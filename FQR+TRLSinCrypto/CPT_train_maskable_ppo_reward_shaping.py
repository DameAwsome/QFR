# -*- coding: utf-8 -*-
# train_maskable_ppo_reward_shaping_original.py

import os
import json
import signal
import pickle
from typing import Tuple
import numpy as np
import pandas as pd
import torch

from sb3cr_contrib.ppo_reward_shaping.ppo_reward_shaping import Maskable_ppo_reward_shaping
from stable_baselines3.common.callbacks import BaseCallback

from alphagen.data.expression import *           # è¡¨è¾¾å¼ç®—å­
from alphagen.models.alpha_pool import AlphaPool, AlphaPoolBase
from alphagen.rl.env.wrapper import AlphaEnv
from alphagen.rl.policy import LSTMSharedNet

from crypto_data_calculator import CryptoDataCalculator

from alphagen.rl.policy import LSTMSharedNet as _LSTMSharedNet
import torch as _torch
from numbers import Number
# --- features extractor åŒ…è£…ï¼šåªè´Ÿè´£æŠŠå‚æ•°æŒ‰æ­£ç¡®ä½ç½®ä¼ ç»™ LSTMSharedNet ---
from alphagen.rl.policy import LSTMSharedNet as _LSTMSharedNet
import torch as _torch

class LSTMExtractor(_LSTMSharedNet):
    def __init__(self,
                 observation_space,
                 n_layers: int = 2,
                 d_model: int = 128,
                 dropout: float = 0.1,
                 device: _torch.device = None,
                 **kwargs):
        if device is None:
            device = _torch.device('cuda:0') if _torch.cuda.is_available() else _torch.device('cpu')
        # ä¸¥æ ¼æŒ‰ä½œè€…ç­¾åï¼š(obs, n_layers, d_model, dropout, device)
        super().__init__(observation_space, int(n_layers), int(d_model), float(dropout), device)


def _to_int(x, default):
    try:
        if isinstance(x, Number):
            return int(x)
        return int(float(x))
    except Exception:
        return int(default)

def _to_float(x, default):
    try:
        if isinstance(x, Number):
            return float(x)
        return float(x)
    except Exception:
        return float(default)

class WrappedLSTMPolicy(_LSTMSharedNet):
    """
    é€‚é…ç®—æ³•ç­¾å (observation_space, action_space, **kwargs)
    å¹¶å¯¹ n_layers/d_model/dropout/device åšæ¸…æ´—ä¸å…œåº•ï¼Œé˜²æ­¢è¢«ä¸Šå±‚ kwargs è¦†ç›–ä¸ºéæ³•å¯¹è±¡ã€‚
    """
    def __init__(self,
                 observation_space,
                 action_space,                 # å¿½ç•¥ï¼Œä¸ä¸Šä¼ 
                 n_layers=2,
                 d_model=128,
                 dropout=0.1,
                 device=None,
                 **kwargs):

        # --- æ¸…æ´—ï¼šå¦‚æœè¢«ä¸Šå±‚ kwargs è¦†ç›–ä¸ºå¥‡æ€ªç±»å‹ï¼ˆæ¯”å¦‚å‡½æ•°ï¼‰ï¼Œå¼ºåˆ¶å›è½ä¸ºé»˜è®¤ ---
        n_layers = _to_int(n_layers, 2) if not callable(n_layers) else 2
        d_model  = _to_int(d_model, 128) if not callable(d_model) else 128
        dropout  = _to_float(dropout, 0.1) if not callable(dropout) else 0.1

        if device is None or callable(device):
            device = _torch.device('cuda:0') if _torch.cuda.is_available() else _torch.device('cpu')

        # é¿å…åŒåè„é”®å†é€šè¿‡ kwargs ä¼ ç»™çˆ¶ç±»
        for k in ["n_layers", "d_model", "dropout", "device"]:
            if k in kwargs:
                kwargs.pop(k, None)

        # å…³é”®ï¼šä¸¥æ ¼ç”¨â€œä½ç½®å‚æ•°â€æŒ‰ä½œè€…é¡ºåºä¸Šä¼ ï¼Œå½»åº•æœç»é”™ä½
        super().__init__(observation_space, n_layers, d_model, dropout, device)

# ===== å›è°ƒï¼šæ¯ä¸ª rollout è®°å½•å¹¶æ‰“å°å…³é”®æŒ‡æ ‡ã€å¹¶è½ CSV =====
from stable_baselines3.common.callbacks import BaseCallback
import time

class CustomCallback(BaseCallback):
    def __init__(
        self,
        pool,                     # AlphaPool å®ä¾‹ï¼ˆè®­ç»ƒç”¨çš„é‚£ä¸€ä¸ªï¼‰
        test_calculator,          # ç”¨äºè¯„ä¼°çš„ test calculator
        save_path,                # ä¿å­˜ CSV çš„ç›®å½•
        name_prefix="crypto",
        show_freq_rollouts=1,     # æ¯å¤šå°‘ä¸ª rollout æ‰“å°ä¸€æ¬¡ï¼›=1 å°±æ˜¯"æ¯ä¸ª"
        verbose=0,
        train_calculator=None,    # ç”¨äºå› å­éªŒè¯çš„è®­ç»ƒè®¡ç®—å™¨ï¼ˆå¯é€‰ï¼‰
        valid_calculator=None,    # ç”¨äºå› å­éªŒè¯çš„éªŒè¯è®¡ç®—å™¨ï¼ˆå¯é€‰ï¼‰
        save_factors_freq=50,     # æ¯å¤šå°‘ä¸ª rollout ä¿å­˜ä¸€æ¬¡å› å­ï¼ˆ0=ä¸ä¿å­˜ï¼Œä»…åœ¨ç»“æŸæ—¶ä¿å­˜ï¼‰
    ):
        super().__init__(verbose)
        self.pool = pool
        self.test_calculator = test_calculator
        self.save_path = save_path
        self.name_prefix = name_prefix
        self.show_freq_rollouts = max(1, int(show_freq_rollouts))
        self.train_calculator = train_calculator
        self.valid_calculator = valid_calculator
        self.save_factors_freq = int(save_factors_freq) if save_factors_freq > 0 else 0

        self._rollout_counter = 0
        self._csv_dir = None
        self._csv_file = None
        self._ts_tag = time.strftime("%Y%m%d_%H%M%S")

    def _on_training_start(self) -> None:
        # æ—¥å¿—ç›®å½•
        os.makedirs(self.save_path, exist_ok=True)
        self._csv_dir = os.path.join(self.save_path, f"{self.name_prefix}_{self._ts_tag}")
        os.makedirs(self._csv_dir, exist_ok=True)
        self._csv_file = os.path.join(self._csv_dir, "rollout_log.csv")
        # å†™ CSV å¤´
        with open(self._csv_file, "w") as f:
            f.write("num_timesteps,rollout_idx,pool_size,best_ic_ret,test_ic,test_long_short\n")

    def _on_rollout_end(self) -> None:
        # è¯„ä¼°ï¼ˆæµ‹è¯•é›†ï¼‰
        try:
            test_ic, test_ls = self.pool.test_ensemble(self.test_calculator)
        except Exception:
            # æŸäº›æ—©æœŸé˜¶æ®µå¯èƒ½è¯„ä¼°å¤±è´¥ï¼Œç»™ä¸ª NaN å¡«ä½
            test_ic, test_ls = float("nan"), float("nan")

        # TensorBoard è®°å½•
        assert self.logger is not None
        self.logger.record("pool/size", self.pool.size)
        # æœ‰çš„å®ç°é‡Œ best_ic_ret å¯èƒ½ä¸å­˜åœ¨ï¼›åšä¸ªç¨³å¦¥çš„ getattr
        best_ic_ret = float(getattr(self.pool, "best_ic_ret", float("nan")))
        self.logger.record("pool/best_ic_ret", best_ic_ret)
        self.logger.record("test/ic", test_ic)
        self.logger.record("test/long_short_return", test_ls)

        # ç´¯è®¡ rollout æ¬¡æ•°
        self._rollout_counter += 1

        # æ§åˆ¶å°æ‰“å°ï¼ˆæŒ‰é¢‘ç‡ï¼‰
        if self._rollout_counter % self.show_freq_rollouts == 0:
            print(
                f"[rollout_end] steps={self.num_timesteps} | "
                f"pool.size={self.pool.size} | "
                f"best_ic_ret={best_ic_ret:.6f} | "
                f"test.ic={test_ic:.6f} | "
                f"test.long_short={test_ls:.6f}"
            )

        # è¿½åŠ åˆ° CSV
        try:
            with open(self._csv_file, "a") as f:
                f.write(
                    f"{self.num_timesteps},{self._rollout_counter},"
                    f"{self.pool.size},{best_ic_ret:.6f},{test_ic:.6f},{test_ls:.6f}\n"
                )
        except Exception as e:
            print("[warn] å†™å…¥ rollout_log.csv å¤±è´¥ï¼š", e)
        
        # === å®šæœŸä¿å­˜å› å­ï¼ˆå¦‚æœé…ç½®äº†ï¼‰===
        if (self.save_factors_freq > 0 and 
            self._rollout_counter % self.save_factors_freq == 0 and 
            self.pool.size > 0 and
            self.train_calculator is not None and 
            self.valid_calculator is not None):
            try:
                print(f"\nğŸ’¾ [è‡ªåŠ¨ä¿å­˜] åœ¨ç¬¬ {self._rollout_counter} ä¸ª rollout æ—¶ä¿å­˜å› å­ (æ± å¤§å°: {self.pool.size})...")
                # åˆ›å»ºä¸´æ—¶ä¿å­˜ç›®å½•
                temp_save_dir = os.path.join(self.save_path, f"{self.name_prefix}_{self._ts_tag}_checkpoints")
                os.makedirs(temp_save_dir, exist_ok=True)
                # ä¿å­˜å› å­ï¼ˆä¸è¿›è¡Œå®Œæ•´éªŒè¯ï¼Œåªä¿å­˜åŸºæœ¬ä¿¡æ¯ï¼‰
                self._quick_save_factors(temp_save_dir, self._rollout_counter)
                print(f"âœ… å› å­å·²ä¿å­˜åˆ°: {temp_save_dir}\n")
            except Exception as e:
                print(f"[warn] å®šæœŸä¿å­˜å› å­å¤±è´¥ï¼š{e}")
        
        # === æ—©æœŸæ’­ç§ï¼ˆè‡ªåŠ¨æ¢æµ‹å¯ç”¨çš„åŠ å…¥æ–¹æ³•åï¼‰===
        try:
            if getattr(self.pool, "size", 0) == 0:
                class _SeedExpr:
                    def evaluate(self, data, period):
                        close = data.get("close", period)
                        if close.shape[0] < 4:
                            return torch.zeros_like(close)
                        mom = close[3:] / (close[:-3] + 1e-12) - 1.0
                        pad = torch.zeros((3, close.shape[1]), dtype=close.dtype, device=close.device)
                        return torch.cat([pad, mom], dim=0)

                ic_seed = self.pool.calculator.calc_single_IC_ret(_SeedExpr())
                if np.isfinite(ic_seed):
                    # 1) ä¼˜å…ˆå°è¯•â€œçœ‹èµ·æ¥åƒåŠ å…¥â€çš„æ–¹æ³•å
                    method_candidates = []
                    for name in dir(self.pool):
                        lname = name.lower()
                        if any(k in lname for k in ["add", "accept", "insert", "push", "submit"]):
                            if callable(getattr(self.pool, name)):
                                method_candidates.append(name)
                    method_candidates.extend(["add_expr", "add", "add_candidate", "add_last_candidate", "accept"])

                    used = False
                    for m in dict.fromkeys(method_candidates):  # å»é‡ä¿åº
                        if hasattr(self.pool, m):
                            try:
                                fn = getattr(self.pool, m)
                                # å¸¸è§ç­¾åï¼šfn(expr) / fn(expr, **kwargs)
                                res = fn(_SeedExpr())
                                print(f"[early-seed] tried {m}, result={res}")
                                used = True
                                break
                            except TypeError:
                                # æœ‰çš„ç­¾åå¯èƒ½æ˜¯ (expr, stats)ï¼›ç»™æœ€å°å…œåº•
                                try:
                                    res = fn(_SeedExpr(), None)
                                    print(f"[early-seed] tried {m}(expr, None), result={res}")
                                    used = True
                                    break
                                except Exception as e2:
                                    print(f"[early-seed] {m} signature mismatch:", e2)
                            except Exception as e:
                                print(f"[early-seed] {m} failed:", e)

                    if not used:
                        # 2) å¦‚æœæ²¡æœ‰ä»»ä½•â€œåŠ å…¥â€æ–¹æ³•ï¼Œå°±å°è¯•æ‰¾â€œå€™é€‰é˜Ÿåˆ—â€å±æ€§åï¼Œç›´æ¥ append
                        for attr in ["candidates", "candidate_queue", "queue", "buffer"]:
                            if hasattr(self.pool, attr):
                                try:
                                    q = getattr(self.pool, attr)
                                    if hasattr(q, "append"):
                                        q.append(_SeedExpr())
                                        print(f"[early-seed] appended seed to pool.{attr}")
                                        used = True
                                        break
                                except Exception as e:
                                    print(f"[early-seed] append to {attr} failed:", e)

                    if not used:
                        print("[early-seed] still no usable hook on pool; will rely on relaxed thresholds only.")
        except Exception as e:
            print("[early-seed] seeding error:", e)

    def _quick_save_factors(self, save_dir, rollout_idx):
        """å¿«é€Ÿä¿å­˜å› å­ï¼ˆä¸è¿›è¡Œå®Œæ•´éªŒè¯ï¼Œåªä¿å­˜åŸºæœ¬ä¿¡æ¯ï¼‰"""
        import pickle
        from datetime import datetime
        
        factors_info = []
        for i in range(self.pool.size):
            try:
                expr = self.pool.exprs[i] if hasattr(self.pool, 'exprs') else None
                weight = self.pool.weights[i] if hasattr(self.pool, 'weights') else 0.0
                
                if expr is None:
                    continue
                
                # åªè®¡ç®—æµ‹è¯•é›†çš„ ICï¼ˆå¿«é€Ÿï¼‰
                try:
                    ic, ls = self.test_calculator.calc_single_IC_ret_with_ls(expr)
                except:
                    ic, ls = float('nan'), float('nan')
                
                factor_info = {
                    'index': i,
                    'weight': float(weight),
                    'expression_str': str(expr),
                    'test_ic': float(ic) if np.isfinite(ic) else np.nan,
                    'test_long_short': float(ls) if np.isfinite(ls) else np.nan,
                    'rollout_idx': rollout_idx,
                    'timestamp': datetime.now().isoformat()
                }
                factors_info.append(factor_info)
            except Exception as e:
                pass
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        checkpoint_file = os.path.join(save_dir, f"factors_checkpoint_rollout_{rollout_idx}.pkl")
        with open(checkpoint_file, 'wb') as f:
            pickle.dump(factors_info, f)
        
        # ä¹Ÿä¿å­˜ä¸€ä¸ª CSV æ‘˜è¦
        if factors_info:
            import pandas as pd
            df_data = []
            for info in factors_info:
                df_data.append({
                    'index': info['index'],
                    'weight': info['weight'],
                    'test_ic': info['test_ic'],
                    'test_long_short': info['test_long_short'],
                    'expression': info['expression_str']
                })
            df = pd.DataFrame(df_data)
            csv_file = os.path.join(save_dir, f"factors_checkpoint_rollout_{rollout_idx}.csv")
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')

    def _on_step(self) -> bool:
        # ä¸éœ€è¦é€æ­¥é€»è¾‘çš„è¯ï¼Œè¿”å› True è®©è®­ç»ƒç»§ç»­
        return True


    # ï¼ˆå¯é€‰ï¼‰æƒ³çœ‹æ± é‡Œæ¯æ¡è¡¨è¾¾å¼çš„æƒé‡/è¡¨ç°ï¼Œå¯ä»¥è¡¥ä¸€ä¸ªæ–¹æ³•ï¼š
    def show_pool_state(self, topk=10):
        try:
            # ä½ çš„ AlphaPool è‹¥æœ‰å¯¼å‡ºå­—ä¸²çš„æ–¹æ³•å¯ç›´æ¥ç”¨ï¼›å¦åˆ™è¿™é‡Œåªç¤ºæ„
            print(f"[pool] size={self.pool.size}")
            # ä¾‹å¦‚ï¼šprint(self.pool.dumps(topk=topk))
        except Exception:
            pass


# ====== 1) ä¸“å®¶ç¤ºèŒƒè·¯å¾„ï¼ˆæ”¹æˆä½ çš„ç»å¯¹è·¯å¾„ï¼‰======
EXPERT_DEMO_PATH = r"C:\Users\æ±Ÿå°šéœ–\Desktop\QuantFactor\TRLSinCrypto\sb3_contrib\ppo_reward_shaping\expert_demo_crypto_15m.pkl"

# ====== 2) ç›®æ ‡å‡½æ•°ï¼ˆç­‰ä»·äº Ref(close,-6)/Ref(close,-1)-1ï¼‰======
def target_fn(fields):
    c = fields["close"]
    return c.shift(-6) / c - 1

# ====== 3) ä» util_for_expert_demo æ„é€  panelï¼ˆä¸¥æ ¼æ²¿ç”¨ util çš„è¾“å…¥ç»“æ„ï¼‰======
def build_panel_from_futures() -> dict:
    """ç›´æ¥ä» futures ç›®å½•è¯»å–å…­å­—æ®µé¢æ¿æ•°æ®"""
    import os
    import glob
    import pandas as pd
    import numpy as np
    
    DATA_DIR = "/Users/mac/Downloads/QFR/futures"
    TF = "15m"
    
    # è¯»å–æ‰€æœ‰ feather æ–‡ä»¶
    files = sorted(glob.glob(os.path.join(DATA_DIR, f"*{TF}*.feather")))
    if not files:
        raise FileNotFoundError(f"æœªæ‰¾åˆ° {DATA_DIR} ä¸‹å« '{TF}' çš„ feather æ–‡ä»¶")
    
    print(f"æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")
    
    # è¯»å–å¹¶åˆå¹¶æ•°æ®
    dfs = []
    for f in files:
        df = pd.read_feather(f)
        cols = {c.lower(): c for c in df.columns}
        
        # å®¹é”™å¤„ç†åˆ—å
        ts = cols.get('timestamp', cols.get('date'))
        sym = cols.get('symbol', cols.get('instrument', cols.get('code')))
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ—¶é—´æˆ³åˆ—ï¼Œè·³è¿‡
        if ts is None:
            print(f"è­¦å‘Š: {f} ç¼ºå°‘æ—¶é—´æˆ³åˆ—")
            continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç¬¦å·åˆ—ï¼Œä»æ–‡ä»¶åæå–
        if sym is None:
            # ä»æ–‡ä»¶åæå–ç¬¦å· (ä¾‹å¦‚: "1INCH_USDT_USDT-15m-futures.feather" -> "1INCH")
            basename = os.path.basename(f)
            symbol_name = basename.split('_')[0].split('-')[0]
            df['symbol_from_file'] = symbol_name
            sym = 'symbol_from_file'
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—
        required_cols = ['open', 'high', 'low', 'close', 'volume']
        missing_cols = []
        for col in required_cols:
            if col not in cols:
                missing_cols.append(col)
        
        if missing_cols:
            print(f"è­¦å‘Š: {f} ç¼ºå°‘åˆ—: {missing_cols}")
            continue
            
        # é€‰æ‹©éœ€è¦çš„åˆ—
        needed_cols = [ts, sym] + [cols[col] for col in required_cols]
        df = df[needed_cols].copy()
        df.columns = ['timestamp', 'symbol'] + required_cols
        
        # å¤„ç†æ—¶é—´æˆ³
        if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):
            df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True, errors='coerce')
        
        dfs.append(df)
    
    if not dfs:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®æ–‡ä»¶")
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    big = pd.concat(dfs, ignore_index=True).dropna(subset=['timestamp', 'symbol'])
    big = big.sort_values(['timestamp', 'symbol'])
    
    # åˆ›å»ºå…­å­—æ®µé¢æ¿
    panel = {}
    for field in ['open', 'high', 'low', 'close', 'volume']:
        panel[field] = big.pivot(index='timestamp', columns='symbol', values=field)
        panel[field] = panel[field].replace([np.inf, -np.inf], np.nan).ffill().bfill()
        panel[field] = panel[field].dropna(axis=1, how='all')
    
    # è®¡ç®— VWAP (Volume Weighted Average Price)
    if 'volume' in panel and 'close' in panel:
        panel['vwap'] = panel['close']  # ç®€åŒ–ç‰ˆï¼Œå®é™…åº”è¯¥æ˜¯ (high+low+close)/3
    
    # ç¡®ä¿æ‰€æœ‰é¢æ¿æœ‰ç›¸åŒçš„ç´¢å¼•å’Œåˆ—
    base_idx = panel['close'].index.sort_values()
    base_cols = panel['close'].columns
    
    for field in panel:
        panel[field] = panel[field].reindex(index=base_idx, columns=base_cols)
    
    print(f"é¢æ¿æ•°æ®: {len(base_idx)} ä¸ªæ—¶é—´ç‚¹, {len(base_cols)} ä¸ªåˆçº¦")
    print(f"åˆçº¦: {list(base_cols)}")
    
    return panel

def split_time_by_days(idx: pd.DatetimeIndex, train_days=60, valid_days=15, test_days=15):
    idx = idx.sort_values()
    # æå–å”¯ä¸€æ—¥æœŸï¼ˆå»é™¤æ—¶åŒºä¿¡æ¯ï¼Œåªä¿ç•™æ—¥æœŸéƒ¨åˆ†ï¼‰
    days = idx.normalize().unique()
    if len(days) < train_days + valid_days + test_days:
        raise ValueError("å¯ç”¨å¤©æ•°ä¸è¶³ä»¥æŒ‰ 60/15/15 åˆ‡åˆ†")
    def day_end(d): return pd.Timestamp(d) + pd.Timedelta(hours=23, minutes=59, seconds=59)
    train_start, train_end = pd.Timestamp(days[0]), day_end(days[train_days-1])
    valid_start, valid_end = pd.Timestamp(days[train_days]), day_end(days[train_days+valid_days-1])
    test_start,  test_end  = pd.Timestamp(days[train_days+valid_days]), day_end(days[train_days+valid_days+test_days-1])
    
    # ç¡®ä¿è¿”å›çš„æ—¶é—´æˆ³ä¸åŸç´¢å¼•æ—¶åŒºåŒ¹é…ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
    return (train_start, train_end), (valid_start, valid_end), (test_start, test_end)

def make_calculators_from_panel(panel: dict):
    close_idx = panel["close"].index
    print(f"é¢æ¿æ•°æ®æ€»æ—¥æœŸæ•°: {len(close_idx)}, å”¯ä¸€æ—¥æœŸæ•°: {len(close_idx.normalize().unique())}")
    (tr_s, tr_e), (va_s, va_e), (te_s, te_e) = split_time_by_days(close_idx, 60, 15, 15)
    print(f"è®­ç»ƒé›†: {tr_s} åˆ° {tr_e}")
    print(f"éªŒè¯é›†: {va_s} åˆ° {va_e}")
    print(f"æµ‹è¯•é›†: {te_s} åˆ° {te_e}")
    calc_train = CryptoDataCalculator(panel, target_fn=target_fn, start=tr_s, end=tr_e)
    print(f"è®­ç»ƒé›†åˆ›å»ºæˆåŠŸï¼Œæ ·æœ¬æ•°: {len(calc_train._stock_data.index)}")
    calc_valid = CryptoDataCalculator(panel, target_fn=target_fn, start=va_s, end=va_e)
    print(f"éªŒè¯é›†åˆ›å»ºæˆåŠŸï¼Œæ ·æœ¬æ•°: {len(calc_valid._stock_data.index)}")
    calc_test  = CryptoDataCalculator(panel, target_fn=target_fn, start=te_s, end=te_e)
    print(f"æµ‹è¯•é›†åˆ›å»ºæˆåŠŸï¼Œæ ·æœ¬æ•°: {len(calc_test._stock_data.index)}")
    return calc_train, calc_valid, calc_test

def main(seed: int = 5, pool: int = 20, steps: int = 600_000):
    np.random.seed(seed)
    torch.manual_seed(seed)

    panel = build_panel_from_futures()
    calc_train, calc_valid, calc_test = make_calculators_from_panel(panel)
        # === probes: è§¦å‘å¸¦ç»Ÿè®¡ç‰ˆ IC è®¡ç®— + æ£€æŸ¥è¡¨è¾¾å¼è¾“å‡ºç»´åº¦ ===
    class _NaiveCloseMomExpr:
        def evaluate(self, data, period):
            close = data.get("close", period)   # æœŸæœ› [T, N]
            if close.shape[0] < 4:
                return torch.zeros_like(close)
            mom = close[3:] / (close[:-3] + 1e-12) - 1.0
            pad = torch.zeros((3, close.shape[1]), dtype=close.dtype, device=close.device)
            return torch.cat([pad, mom], dim=0)

    ic_probe, ls_probe = calc_train.calc_single_IC_ret_with_ls(_NaiveCloseMomExpr())
    print(f"[probe] naive expr => IC={ic_probe}, LS={ls_probe}")

    try:
        df_probe = calc_train._eval_expr(_NaiveCloseMomExpr())
        print("[expr-shape] naive expr matrix shape =", df_probe.shape)  # æœŸæœ› (T, N)
    except Exception as e:
        print("[expr-shape] evaluate failed:", e)


        # === æ•°æ®å¥åº·åº¦è‡ªæ£€ï¼šç›®æ ‡è¦†ç›–ç‡ + æ¨ªæˆªé¢æ ·æœ¬é‡ ===
    try:
        y_tr = calc_train.get_target()  # DataFrame: index=æ—¶é—´, columns=åˆçº¦
        notna_per_day = y_tr.notna().sum(axis=1)
        print("[data-check] train days =", len(y_tr.index),
              "symbols =", y_tr.shape[1],
              "median xsec count =", int(notna_per_day.median()),
              "p10 =", int(notna_per_day.quantile(0.1)),
              "p90 =", int(notna_per_day.quantile(0.9)),
              "overall_nan_ratio =", float(y_tr.isna().mean().mean()))
    except Exception as e:
        print("[data-check][ERROR] è¯»å–è®­ç»ƒç›®æ ‡å¤±è´¥ï¼š", e)

    # === åŸºçº¿å› å­è‡ªæ£€ï¼šç”¨ very-simple baseline å› å­ç®—ä¸€æ¬¡ ICï¼Œçœ‹çœ‹æ˜¯å¦èƒ½å¾—åˆ°é NaN ===
    try:
        base_res = calc_train.debug_baselines()  # ä¸‹é¢è¡¥ä¸ B ä¼šåœ¨ calculator é‡Œæä¾›
        print("[baseline-IC] 1) past_1bar_ret IC =", f"{base_res.get('ic_past1', 'NA')}")
        print("[baseline-IC] 2) zscore(close)   IC =", f"{base_res.get('ic_zclose', 'NA')}")
        print("[baseline-IC] 3) zscore(vwap)    IC =", f"{base_res.get('ic_zvwap', 'NA')}")
    except Exception as e:
        print("[baseline-IC][WARN] æ— æ³•è®¡ç®—åŸºçº¿ ICï¼ˆçœ‹èµ·æ¥è®¡ç®—å™¨å­—æ®µ/ç»“æ„å¼‚å¸¸ï¼‰ï¼š", e)


    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')

    alpha_pool = AlphaPool(
        capacity=pool,
        calculator=calc_train,
        device=device,
    )

        # === æš‚æ—¶æ”¾å®½ AlphaPool çš„æ¥æ”¶é˜ˆå€¼ï¼ˆæœ‰åˆ™æ”¹ã€æ— åˆ™è·³ï¼‰===
    relax_kv = [
        ("min_eval_points", 10),          # å•æ—¶ç‚¹æœ€å°‘æ ·æœ¬æ•°
        ("min_n", 3),                     # é€æ—¶ç‚¹ min_n
        ("min_train_ic", -1.0),           # è®­ç»ƒ IC ä¸‹é™
        ("min_train_rankic", -1.0),       # è®­ç»ƒ RankIC ä¸‹é™
        ("min_valid_ic", -1.0),           # éªŒè¯ IC ä¸‹é™
        ("min_coverage", 1),              # è¦†ç›–çš„æœ€å°‘æ—¶ç‚¹æ•°
        ("require_positive_ls", False),   # æ˜¯å¦å¿…é¡»å¤šç©º>0
        ("require_finite_valid", False),  # æ˜¯å¦è¦æ±‚éªŒè¯é›†å…¨ finite
        ("accept_negative_ic", True),     # å…è®¸è´ŸICï¼ˆå…ˆæ”¶è¿›æ¥ï¼Œåç»­å†æ·˜æ±°ï¼‰
    ]
    for k, v in relax_kv:
        if hasattr(alpha_pool, k):
            try:
                setattr(alpha_pool, k, v)
                print(f"[relax] set {k} = {v}")
            except Exception as e:
                print(f"[relax] fail {k}: {e}")

    from sb3cr_contrib.common.maskable.policies import MaskableActorCriticPolicy
    env = AlphaEnv(alpha_pool)
    policy = MaskableActorCriticPolicy

    policy_kwargs = {
        "features_extractor_class": LSTMExtractor,
        "features_extractor_kwargs": {
            "n_layers": 2,
            "d_model": 128,
            "dropout": 0.1,
            "device": device,
        },
    }

    # === æ–°å¢ï¼šTensorBoard æ ¹ç›®å½•ï¼ˆå¯æŒ‰ä½ çš„é¡¹ç›®è·¯å¾„æ”¹ï¼‰===
    current_dir = os.path.dirname(os.path.abspath(__file__))
    tb_root = os.path.join(current_dir, "tensorboard", "ppo_reward_shaping_crypto")
    os.makedirs(tb_root, exist_ok=True)

    # === è¿™é‡Œæ˜ç¡®æŠŠ device ä¼ ç»™ç®—æ³•ä½“ï¼Œå¹¶æ‰“å¼€ tensorboard_log ===
    algo = Maskable_ppo_reward_shaping(
        policy,
        env,
        gamma=1.0,
        policy_kwargs=policy_kwargs,
        device=device,
        tensorboard_log=tb_root,
        verbose=1,
    )

    # === æ–°å¢ï¼šç«‹åˆ»æ£€æŸ¥å¹¶æ‰“å°â€œåˆ°åº•æœ‰æ²¡æœ‰ç”¨ä¸Š GPUâ€===
    print("[device-check] torch.cuda.is_available =", torch.cuda.is_available())
    if torch.cuda.is_available():
        print("[device-check] cuda name =", torch.cuda.get_device_name(0))
    print("[device-check] policy.device =", algo.policy.device)
    # å…¼å®¹ï¼šæœ‰ CUDA ç”¨ CUDAï¼›å¦åˆ™å°è¯• MPSï¼›éƒ½ä¸å¯ç”¨æ—¶ä½¿ç”¨ CPU
    if torch.cuda.is_available():
        if not str(algo.policy.device).startswith("cuda"):
            try:
                algo.policy.to("cuda")
                print(f"[OK] å·²è¿ç§»åˆ° {algo.policy.device}")
            except Exception as e:
                print(f"[WARN] CUDA è¿ç§»å¤±è´¥ï¼Œå°†ç»§ç»­ä½¿ç”¨ {algo.policy.device}: {e}")
    else:
        # ä¸ºç¨³å®šæ€§ç¦ç”¨ MPS å›é€€ï¼Œç›´æ¥ä½¿ç”¨ CPUï¼ˆé¿å… MPS åœ¨ nn.Embedding ç­‰ç®—å­ä¸Šçš„å·²çŸ¥é—®é¢˜ï¼‰
        if str(algo.policy.device).startswith("mps"):
            try:
                algo.policy.to("cpu")
                print("[INFO] å·²ä» MPS å›é€€åˆ° CPU ä»¥é¿å…ä¸å…¼å®¹ç®—å­é—®é¢˜")
            except Exception as e:
                print(f"[WARN] å›é€€ CPU å¤±è´¥ï¼Œä»åœ¨ {algo.policy.device}: {e}")
        else:
            print("[INFO] æ—  CUDAï¼Œä½¿ç”¨ CPU è®­ç»ƒ")

    # === æ–°å¢ï¼šæŒ‚ä¸Šæˆ‘ä»¬çš„å›è°ƒï¼ˆæ¯ä¸ª rollout éƒ½æ‰“å°ï¼›æƒ³é™é¢‘å°±æŠŠ show_freq_rollouts æ”¹æˆ >1ï¼‰===
    save_dir = os.path.join(current_dir, "checkpoints", "ppo_reward_shaping_crypto")
    callback = CustomCallback(
        pool=alpha_pool,
        test_calculator=calc_test,
        save_path=save_dir,
        name_prefix=f"pool{pool}_seed{seed}",
        show_freq_rollouts=1,    # æ¯ä¸ª rollout æ‰“å°ï¼›æ”¹æˆ 5 è¡¨ç¤ºæ¯ 5 ä¸ª rollout æ‰“å°ä¸€æ¬¡
        train_calculator=calc_train,  # ç”¨äºå®šæœŸä¿å­˜å› å­ï¼ˆå¯é€‰ï¼‰
        valid_calculator=calc_valid,  # ç”¨äºå®šæœŸä¿å­˜å› å­ï¼ˆå¯é€‰ï¼‰
        save_factors_freq=0,    # å®šæœŸä¿å­˜é¢‘ç‡ï¼ˆè®¾ä¸º 0 ç¦ç”¨ï¼Œ>0 è¡¨ç¤ºæ¯ N ä¸ª rollout ä¿å­˜ä¸€æ¬¡ï¼‰
    )

    # === æ·»åŠ ä¸­æ–­æ—¶ä¿å­˜å› å­æ± çš„åŠŸèƒ½ ===
    def save_pool_on_interrupt(signum, frame):
        """ä¸­æ–­æ—¶ä¿å­˜å› å­æ± """
        print(f"\nâš ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜å› å­æ± ...")
        pool_save_file = os.path.join(save_dir, f"alpha_pool_seed{seed}_interrupted.pkl")
        try:
            with open(pool_save_file, 'wb') as f:
                pickle.dump({
                    'alpha_pool': alpha_pool,
                    'calc_train': calc_train,
                    'calc_valid': calc_valid,
                    'calc_test': calc_test,
                    'save_dir': save_dir,
                    'seed': seed,
                    'pool_size': pool,
                    'interrupted': True
                }, f)
            print(f"ğŸ’¾ å› å­æ± å·²ä¿å­˜åˆ°: {pool_save_file}")
            print(f"   å¯ä»¥ä½¿ç”¨æ­¤æ–‡ä»¶æ‰‹åŠ¨æå–å› å­: python extract_factors_manual.py --pool_file {pool_save_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜å› å­æ± å¤±è´¥: {e}")
        raise KeyboardInterrupt
    
    signal.signal(signal.SIGINT, save_pool_on_interrupt)
    signal.signal(signal.SIGTERM, save_pool_on_interrupt)

    # === å­¦ä¹ ï¼štb_log_name ä¼šä½œä¸º TensorBoard ä¸‹çš„å­ç›®å½•åå­— ===
    try:
        algo.learn(total_timesteps=steps, callback=callback, tb_log_name=f"crypto_pool{pool}_seed{seed}")
    except KeyboardInterrupt:
        print("\nâš ï¸  è®­ç»ƒè¢«ä¸­æ–­")
        raise

    # ï¼ˆä¿ç•™ä½ åŸæ¥çš„æœ«å°¾è¯„ä¼°æ‰“å°ï¼‰
    try:
        ic_val, ret_val = alpha_pool.test_ensemble(calc_valid)
        ic_test, ret_test = alpha_pool.test_ensemble(calc_test)
        print(f"[EVAL] valid: IC={ic_val:.4f}, ret={ret_val:.4f} | test: IC={ic_test:.4f}, ret={ret_test:.4f}")
    except Exception as e:
        print("[WARN] è¯„ä¼°é˜¶æ®µå‡ºé”™ï¼ˆå¯å¿½ç•¥è®­ç»ƒå·²å®Œæˆï¼‰:", e)
    
    # === è®­ç»ƒå®Œæˆï¼Œè‡ªåŠ¨æå–å› å­ ===
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼å› å­æ± å¤§å°: {alpha_pool.size}")
    
    # ä¿å­˜ alpha_pool å¯¹è±¡ï¼Œæ–¹ä¾¿åç»­æ‰‹åŠ¨æå–
    pool_save_file = os.path.join(save_dir, f"alpha_pool_seed{seed}_final.pkl")
    try:
        with open(pool_save_file, 'wb') as f:
            pickle.dump({
                'alpha_pool': alpha_pool,
                'calc_train': calc_train,
                'calc_valid': calc_valid,
                'calc_test': calc_test,
                'save_dir': save_dir,
                'seed': seed,
                'pool_size': pool
            }, f)
        print(f"ğŸ’¾ å› å­æ± å¯¹è±¡å·²ä¿å­˜åˆ°: {pool_save_file}")
    except Exception as e:
        print(f"[WARN] ä¿å­˜å› å­æ± å¯¹è±¡å¤±è´¥: {e}")
    
    # è‡ªåŠ¨æå–å› å­
    if alpha_pool.size > 0:
        print(f"\n{'='*80}")
        print(f"å¼€å§‹è‡ªåŠ¨æå–å› å­...")
        print(f"{'='*80}\n")
        try:
            extract_factors(alpha_pool, calc_train, calc_valid, calc_test, save_dir)
            print(f"\nâœ… å› å­æå–å®Œæˆï¼")
            print(f"ğŸ“ å› å­æ–‡ä»¶ä¿å­˜åœ¨: {save_dir}/extracted_factors/")
        except Exception as e:
            print(f"\nâš ï¸  è‡ªåŠ¨æå–å› å­å¤±è´¥: {e}")
            print(f"ğŸ’¡ å¯ä»¥ç¨åæ‰‹åŠ¨è¿è¡Œ: python extract_factors_manual.py --pool_file {pool_save_file}")
            import traceback
            traceback.print_exc()
    else:
        print(f"âš ï¸  å› å­æ± ä¸ºç©ºï¼Œè·³è¿‡å› å­æå–")

def extract_factors(alpha_pool, train_calculator, valid_calculator, test_calculator, save_dir):
    """æå–å¹¶ä¿å­˜è®­ç»ƒç”Ÿæˆçš„å› å­ï¼Œå¹¶è¿›è¡Œè´¨é‡éªŒè¯"""
    import os
    import pickle
    from datetime import datetime
    
    print(f"\n{'='*80}")
    print(f"ğŸ“¦ å¼€å§‹æå–å› å­ (å› å­æ± å¤§å°: {alpha_pool.size})")
    print(f"{'='*80}\n")
    
    if alpha_pool.size == 0:
        print("âš ï¸  è­¦å‘Š: å› å­æ± ä¸ºç©ºï¼Œæ²¡æœ‰å› å­å¯æå–")
        return None
    
    # åˆ›å»ºå› å­ä¿å­˜ç›®å½•
    factors_dir = os.path.join(save_dir, "extracted_factors")
    os.makedirs(factors_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # å…ˆè¿›è¡Œå› å­è´¨é‡åˆ†æ
    print("ğŸ” æ­£åœ¨è¿›è¡Œå› å­è´¨é‡åˆ†æ...\n")
    analysis_df = analyze_factor_pool(
        alpha_pool, train_calculator, valid_calculator, test_calculator, top_k=min(10, alpha_pool.size)
    )
    
    # ä¿å­˜åˆ†æç»“æœ
    if not analysis_df.empty:
        analysis_file = os.path.join(factors_dir, f"factor_analysis_{timestamp}.csv")
        analysis_df.to_csv(analysis_file, index=False, encoding='utf-8-sig')
        print(f"\nğŸ“Š å› å­åˆ†æç»“æœå·²ä¿å­˜åˆ°: {analysis_file}\n")
    
    # æå–æ¯ä¸ªå› å­çš„è¯¦ç»†ä¿¡æ¯
    factors_info = []
    
    for i in range(alpha_pool.size):
        try:
            # è·å–å› å­è¡¨è¾¾å¼
            expr = alpha_pool.exprs[i] if hasattr(alpha_pool, 'exprs') else None
            weight = alpha_pool.weights[i] if hasattr(alpha_pool, 'weights') else 0.0
            
            if expr is None:
                continue
            
            # è¿›è¡Œè¯¦ç»†éªŒè¯
            validation_result = validate_factor_quality(
                expr, train_calculator, valid_calculator, test_calculator, verbose=False
            )
            
            # è®¡ç®—å› å­å€¼
            factor_values = test_calculator._eval_expr(expr)
            
            factor_info = {
                'index': i,
                'weight': float(weight),
                'expression_str': str(expr),
                'factor_values_shape': factor_values.shape,
                'factor_values': factor_values,
                'validation': validation_result
            }
            
            factors_info.append(factor_info)
            
        except Exception as e:
            print(f"âš ï¸  å› å­ {i} æå–å¤±è´¥: {e}")
    
    # ä¿å­˜å› å­ä¿¡æ¯ï¼ˆä½¿ç”¨pklæ ¼å¼ï¼‰
    # ä¸ºä»€ä¹ˆç”¨pklè€Œä¸æ˜¯CSVï¼Ÿ
    # 1. å› å­å€¼DataFrameå¾ˆå¤§ï¼ˆæ—¶é—´Ã—åˆçº¦ï¼‰ï¼Œpklå¯ä»¥å®Œæ•´ä¿å­˜DataFrameå¯¹è±¡ï¼ˆåŒ…æ‹¬ç´¢å¼•ã€æ•°æ®ç±»å‹ç­‰ï¼‰
    # 2. åŒ…å«å¤æ‚çš„éªŒè¯ç»“æœå­—å…¸ï¼ŒCSVæ— æ³•å¾ˆå¥½åœ°ä¿å­˜åµŒå¥—ç»“æ„
    # 3. åŠ è½½é€Ÿåº¦å¿«ï¼Œé€‚åˆåç»­ç¨‹åºåŒ–å¤„ç†
    # æ³¨æ„ï¼šå› å­å€¼çŸ©é˜µä¼šå¦å¤–ä¿å­˜ä¸ºCSVæ ¼å¼ï¼ˆé•¿æ ¼å¼ï¼‰ï¼Œä¾¿äºæŸ¥çœ‹å’Œåˆ†æ
    factors_file = os.path.join(factors_dir, f"factors_{timestamp}.pkl")
    with open(factors_file, 'wb') as f:
        pickle.dump(factors_info, f)
    
    print(f"\nâœ… æˆåŠŸæå– {len(factors_info)} ä¸ªå› å­")
    print(f"ğŸ“ å› å­è¯¦ç»†ä¿¡æ¯å·²ä¿å­˜åˆ°: {factors_file} (pklæ ¼å¼ï¼ŒåŒ…å«å®Œæ•´DataFrameå’ŒéªŒè¯ç»“æœ)")
    print(f"ğŸ’¡ æç¤º: ä½¿ç”¨ä»¥ä¸‹ä»£ç åŠ è½½å› å­ä¿¡æ¯ï¼š")
    print(f"   import pickle")
    print(f"   with open('{factors_file}', 'rb') as f:")
    print(f"       factors_info = pickle.load(f)")
    
    # ä¿å­˜å› å­å€¼çŸ©é˜µï¼ˆCSVæ ¼å¼ï¼Œä¾¿äºæŸ¥çœ‹ï¼‰
    if factors_info:
        factor_matrix_file = os.path.join(factors_dir, f"factor_matrix_{timestamp}.csv")
        try:
            # åˆå¹¶æ‰€æœ‰å› å­å€¼
            # æ³¨æ„ï¼šæ¯ä¸ªå› å­çš„factor_valuesæ˜¯ (æ—¶é—´, åˆçº¦) çš„DataFrame
            # æˆ‘ä»¬éœ€è¦å°†æ¯ä¸ªå› å­è½¬æ¢ä¸ºé•¿æ ¼å¼ï¼Œç„¶ååˆå¹¶
            all_factors_long = []
            for info in factors_info:
                factor_df = info['factor_values'].copy()
                # é‡ç½®ç´¢å¼•ï¼Œå°†æ—¶é—´ä½œä¸ºåˆ—
                factor_df = factor_df.reset_index()
                # è½¬æ¢ä¸ºé•¿æ ¼å¼ï¼šdate, symbol, factor_value
                factor_long = factor_df.melt(
                    id_vars=[factor_df.columns[0]],  # ç¬¬ä¸€åˆ—æ˜¯æ—¶é—´ç´¢å¼•
                    var_name='symbol',
                    value_name=f"factor_{info['index']}"
                )
                factor_long.rename(columns={factor_df.columns[0]: 'date'}, inplace=True)
                all_factors_long.append(factor_long)
            
            if all_factors_long:
                # æŒ‰ date å’Œ symbol åˆå¹¶æ‰€æœ‰å› å­
                combined_factors = all_factors_long[0]
                for factor_long in all_factors_long[1:]:
                    combined_factors = pd.merge(
                        combined_factors, 
                        factor_long, 
                        on=['date', 'symbol'], 
                        how='outer'
                    )
                
                combined_factors.to_csv(factor_matrix_file, index=False)
                print(f"ğŸ“Š å› å­å€¼çŸ©é˜µå·²ä¿å­˜åˆ°: {factor_matrix_file}")
                print(f"   æ ¼å¼: é•¿æ ¼å¼ (date, symbol, factor_0, factor_1, ...)")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜å› å­å€¼çŸ©é˜µå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    return factors_info, analysis_df


def validate_factor_quality(
    expr,
    train_calculator,
    valid_calculator,
    test_calculator,
    verbose=True
):
    """
    å…¨é¢éªŒè¯å› å­è´¨é‡ï¼ŒåŒ…æ‹¬ï¼š
    1. IC å’Œ RankIC åˆ†æï¼ˆè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†ï¼‰
    2. å¤šç©ºæ”¶ç›Šåˆ†æ
    3. å› å­åˆ†å¸ƒå’Œç»Ÿè®¡ç‰¹å¾
    4. å› å­ç¨³å®šæ€§ï¼ˆè·¨æ•°æ®é›†ä¸€è‡´æ€§ï¼‰
    5. å› å­å¯è§£é‡Šæ€§
    
    è¿”å›: dict åŒ…å«æ‰€æœ‰éªŒè¯æŒ‡æ ‡
    """
    import numpy as np
    import pandas as pd
    
    results = {
        'expression': str(expr),
        'train': {},
        'valid': {},
        'test': {},
        'stability': {},
        'statistics': {}
    }
    
    # 1. è®¡ç®—å„æ•°æ®é›†çš„ IC å’Œ RankIC
    for name, calc in [('train', train_calculator), ('valid', valid_calculator), ('test', test_calculator)]:
        try:
            # IC å’Œ RankIC
            ic, rank_ic = calc.calc_single_all_ret(expr)
            ic_ls, ls = calc.calc_single_IC_ret_with_ls(expr)
            
            # è®¡ç®—å› å­å€¼
            factor_values = calc._eval_expr(expr)
            target = calc.get_target()
            
            # å¯¹é½æ•°æ®
            factor_values, target = factor_values.align(target, join='inner', axis=0)
            
            # å› å­ç»Ÿè®¡
            factor_flat = factor_values.values.flatten()
            factor_flat = factor_flat[~np.isnan(factor_flat)]
            
            results[name] = {
                'ic': float(ic) if np.isfinite(ic) else np.nan,
                'rank_ic': float(rank_ic) if np.isfinite(rank_ic) else np.nan,
                'long_short_return': float(ls) if np.isfinite(ls) else np.nan,
                'mean': float(np.nanmean(factor_flat)) if len(factor_flat) > 0 else np.nan,
                'std': float(np.nanstd(factor_flat)) if len(factor_flat) > 0 else np.nan,
                'min': float(np.nanmin(factor_flat)) if len(factor_flat) > 0 else np.nan,
                'max': float(np.nanmax(factor_flat)) if len(factor_flat) > 0 else np.nan,
                'nan_ratio': float(np.isnan(factor_values.values).sum() / factor_values.size),
                'valid_points': int((~np.isnan(factor_values.values)).sum())
            }
        except Exception as e:
            if verbose:
                print(f"âš ï¸  è®¡ç®— {name} é›†æŒ‡æ ‡å¤±è´¥: {e}")
            results[name] = {'error': str(e)}
    
    # 2. å› å­ç¨³å®šæ€§åˆ†æ
    try:
        train_ic = results['train'].get('ic', np.nan)
        valid_ic = results['valid'].get('ic', np.nan)
        test_ic = results['test'].get('ic', np.nan)
        
        # IC è¡°å‡ï¼ˆè®­ç»ƒé›† -> éªŒè¯é›† -> æµ‹è¯•é›†ï¼‰
        ic_decay_train_valid = train_ic - valid_ic if np.isfinite(train_ic) and np.isfinite(valid_ic) else np.nan
        ic_decay_valid_test = valid_ic - test_ic if np.isfinite(valid_ic) and np.isfinite(test_ic) else np.nan
        ic_decay_train_test = train_ic - test_ic if np.isfinite(train_ic) and np.isfinite(test_ic) else np.nan
        
        # IC ç¬¦å·ä¸€è‡´æ€§
        ic_sign_consistent = (
            np.sign(train_ic) == np.sign(valid_ic) == np.sign(test_ic)
            if all(np.isfinite([train_ic, valid_ic, test_ic])) else False
        )
        
        results['stability'] = {
            'ic_decay_train_valid': float(ic_decay_train_valid) if np.isfinite(ic_decay_train_valid) else np.nan,
            'ic_decay_valid_test': float(ic_decay_valid_test) if np.isfinite(ic_decay_valid_test) else np.nan,
            'ic_decay_train_test': float(ic_decay_train_test) if np.isfinite(ic_decay_train_test) else np.nan,
            'ic_sign_consistent': bool(ic_sign_consistent),
            'ic_std': float(np.nanstd([train_ic, valid_ic, test_ic])) if all(np.isfinite([train_ic, valid_ic, test_ic])) else np.nan
        }
    except Exception as e:
        if verbose:
            print(f"âš ï¸  ç¨³å®šæ€§åˆ†æå¤±è´¥: {e}")
    
    # 3. å› å­è´¨é‡è¯„åˆ†
    try:
        # åŸºç¡€è¯„åˆ†ï¼ˆ0-100ï¼‰
        score = 0
        
        # IC ç»å¯¹å€¼è¯„åˆ†ï¼ˆ40åˆ†ï¼‰
        avg_ic = np.nanmean([abs(results['train'].get('ic', 0)), 
                             abs(results['valid'].get('ic', 0)), 
                             abs(results['test'].get('ic', 0))])
        if avg_ic > 0.05:
            score += 40
        elif avg_ic > 0.03:
            score += 30
        elif avg_ic > 0.01:
            score += 20
        elif avg_ic > 0:
            score += 10
        
        # ç¨³å®šæ€§è¯„åˆ†ï¼ˆ30åˆ†ï¼‰
        if results['stability'].get('ic_sign_consistent', False):
            score += 15
        decay = abs(results['stability'].get('ic_decay_train_test', 1))
        if decay < 0.01:
            score += 15
        elif decay < 0.03:
            score += 10
        elif decay < 0.05:
            score += 5
        
        # å¤šç©ºæ”¶ç›Šè¯„åˆ†ï¼ˆ20åˆ†ï¼‰
        avg_ls = np.nanmean([results['train'].get('long_short_return', 0),
                             results['valid'].get('long_short_return', 0),
                             results['test'].get('long_short_return', 0)])
        if avg_ls > 0.1:
            score += 20
        elif avg_ls > 0.05:
            score += 15
        elif avg_ls > 0:
            score += 10
        
        # æ•°æ®è´¨é‡è¯„åˆ†ï¼ˆ10åˆ†ï¼‰
        nan_ratio = np.nanmean([results['train'].get('nan_ratio', 1),
                                results['valid'].get('nan_ratio', 1),
                                results['test'].get('nan_ratio', 1)])
        if nan_ratio < 0.01:
            score += 10
        elif nan_ratio < 0.05:
            score += 7
        elif nan_ratio < 0.1:
            score += 5
        
        results['quality_score'] = float(score)
        results['quality_level'] = (
            'ä¼˜ç§€' if score >= 80 else
            'è‰¯å¥½' if score >= 60 else
            'ä¸€èˆ¬' if score >= 40 else
            'è¾ƒå·®' if score >= 20 else
            'å¾ˆå·®'
        )
    except Exception as e:
        if verbose:
            print(f"âš ï¸  è´¨é‡è¯„åˆ†å¤±è´¥: {e}")
    
    # 4. æ‰“å°ç»“æœ
    if verbose:
        print("\n" + "="*80)
        print(f"å› å­éªŒè¯æŠ¥å‘Š: {str(expr)[:60]}...")
        print("="*80)
        
        print("\nğŸ“Š IC å’Œ RankIC åˆ†æ:")
        for name in ['train', 'valid', 'test']:
            data = results[name]
            if 'error' not in data:
                print(f"  {name.upper():6s}: IC={data.get('ic', np.nan):7.4f}, "
                      f"RankIC={data.get('rank_ic', np.nan):7.4f}, "
                      f"LS={data.get('long_short_return', np.nan):7.4f}")
        
        print("\nğŸ“ˆ å› å­ç¨³å®šæ€§:")
        stability = results['stability']
        print(f"  IC è¡°å‡ (è®­ç»ƒ->éªŒè¯): {stability.get('ic_decay_train_valid', np.nan):7.4f}")
        print(f"  IC è¡°å‡ (éªŒè¯->æµ‹è¯•): {stability.get('ic_decay_valid_test', np.nan):7.4f}")
        print(f"  IC è¡°å‡ (è®­ç»ƒ->æµ‹è¯•): {stability.get('ic_decay_train_test', np.nan):7.4f}")
        print(f"  IC ç¬¦å·ä¸€è‡´æ€§: {'âœ…' if stability.get('ic_sign_consistent', False) else 'âŒ'}")
        
        print("\nğŸ“‰ å› å­ç»Ÿè®¡ç‰¹å¾:")
        for name in ['train', 'valid', 'test']:
            data = results[name]
            if 'error' not in data:
                print(f"  {name.upper():6s}: mean={data.get('mean', np.nan):8.4f}, "
                      f"std={data.get('std', np.nan):8.4f}, "
                      f"nan_ratio={data.get('nan_ratio', np.nan):6.2%}")
        
        if 'quality_score' in results:
            print(f"\nâ­ å› å­è´¨é‡è¯„åˆ†: {results['quality_score']:.1f}/100 ({results['quality_level']})")
        
        print("="*80 + "\n")
    
    return results


def analyze_factor_pool(
    alpha_pool,
    train_calculator,
    valid_calculator,
    test_calculator,
    top_k=10
):
    """
    åˆ†æå› å­æ± ä¸­æ‰€æœ‰å› å­çš„è´¨é‡ï¼Œå¹¶è¿”å›æ’å
    
    è¿”å›: DataFrame åŒ…å«æ‰€æœ‰å› å­çš„éªŒè¯ç»“æœ
    """
    import pandas as pd
    
    if alpha_pool.size == 0:
        print("âš ï¸  å› å­æ± ä¸ºç©º")
        return pd.DataFrame()
    
    print(f"\nğŸ” å¼€å§‹åˆ†æå› å­æ± ä¸­çš„ {alpha_pool.size} ä¸ªå› å­...\n")
    
    all_results = []
    
    for i in range(alpha_pool.size):
        expr = alpha_pool.exprs[i]
        weight = alpha_pool.weights[i]
        
        if expr is None:
            continue
        
        print(f"åˆ†æå› å­ {i+1}/{alpha_pool.size}...", end='\r')
        
        try:
            result = validate_factor_quality(
                expr, train_calculator, valid_calculator, test_calculator, verbose=False
            )
            result['index'] = i
            result['weight'] = float(weight)
            all_results.append(result)
        except Exception as e:
            print(f"\nâš ï¸  å› å­ {i} åˆ†æå¤±è´¥: {e}")
    
    if not all_results:
        print("\nâš ï¸  æ²¡æœ‰æˆåŠŸåˆ†æçš„å› å­")
        return pd.DataFrame()
    
    # è½¬æ¢ä¸º DataFrame
    df_data = []
    for r in all_results:
        row = {
            'index': r['index'],
            'weight': r['weight'],
            'expression': r['expression'],
            'train_ic': r['train'].get('ic', np.nan),
            'valid_ic': r['valid'].get('ic', np.nan),
            'test_ic': r['test'].get('ic', np.nan),
            'train_rank_ic': r['train'].get('rank_ic', np.nan),
            'valid_rank_ic': r['valid'].get('rank_ic', np.nan),
            'test_rank_ic': r['test'].get('rank_ic', np.nan),
            'train_ls': r['train'].get('long_short_return', np.nan),
            'valid_ls': r['valid'].get('long_short_return', np.nan),
            'test_ls': r['test'].get('long_short_return', np.nan),
            'ic_decay_train_test': r['stability'].get('ic_decay_train_test', np.nan),
            'ic_sign_consistent': r['stability'].get('ic_sign_consistent', False),
            'quality_score': r.get('quality_score', 0),
            'quality_level': r.get('quality_level', 'æœªçŸ¥')
        }
        df_data.append(row)
    
    df = pd.DataFrame(df_data)
    
    # æŒ‰è´¨é‡è¯„åˆ†æ’åº
    df = df.sort_values('quality_score', ascending=False)
    
    print(f"\nâœ… å®Œæˆåˆ†æï¼Œå…± {len(df)} ä¸ªæœ‰æ•ˆå› å­\n")
    
    # æ˜¾ç¤º Top K
    print(f"ğŸ† Top {min(top_k, len(df))} å› å­:")
    print("="*120)
    for idx, row in df.head(top_k).iterrows():
        print(f"\næ’å {idx+1}: å› å­ #{int(row['index'])} (è´¨é‡è¯„åˆ†: {row['quality_score']:.1f}/100 - {row['quality_level']})")
        print(f"  æƒé‡: {row['weight']:.6f}")
        print(f"  IC: è®­ç»ƒ={row['train_ic']:7.4f}, éªŒè¯={row['valid_ic']:7.4f}, æµ‹è¯•={row['test_ic']:7.4f}")
        print(f"  RankIC: è®­ç»ƒ={row['train_rank_ic']:7.4f}, éªŒè¯={row['valid_rank_ic']:7.4f}, æµ‹è¯•={row['test_rank_ic']:7.4f}")
        print(f"  å¤šç©ºæ”¶ç›Š: è®­ç»ƒ={row['train_ls']:7.4f}, éªŒè¯={row['valid_ls']:7.4f}, æµ‹è¯•={row['test_ls']:7.4f}")
        print(f"  IC è¡°å‡: {row['ic_decay_train_test']:7.4f}, ç¬¦å·ä¸€è‡´æ€§: {'âœ…' if row['ic_sign_consistent'] else 'âŒ'}")
        print(f"  è¡¨è¾¾å¼: {row['expression'][:100]}...")
        print("-"*120)
    
    return df

if __name__ == "__main__":
    for s in [5, 310, 24, 10, 10086]:
        main(seed=s, pool=20, steps=60_000)